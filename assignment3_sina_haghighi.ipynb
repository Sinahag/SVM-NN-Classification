{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A\n",
    "### Support Vector Machine\n",
    "\n",
    "Train a support vector machine using the images from fer2013.csv. Use the Training set for training, and the PrivateTest test set for testing. Report precision, recall, accuracy, F1 score, and create a confusion matrix on the test set, showing the confusions between emotion labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv('fer2013.csv')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_data = data[data['Usage'] == 'Training']\n",
    "test_data = data[data['Usage'] == 'PrivateTest']\n",
    "\n",
    "# format the pixel column into numpy array\n",
    "X_train_pixels = train_data['pixels'].apply(lambda x: np.fromstring(x, dtype=int, sep=' '))\n",
    "X_train = np.vstack(X_train_pixels.values)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "y_train = train_data['emotion']\n",
    "\n",
    "# format the pixel column into numpy array\n",
    "X_test_pixels = test_data['pixels'].apply(lambda x: np.fromstring(x, dtype=int, sep=' '))\n",
    "X_test = np.vstack(X_test_pixels.values)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "y_test = test_data['emotion']\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Train the SVM\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get Scores\n",
    "predicted = svm.predict(X_test)\n",
    "\n",
    "f1_score = f1_score(y_test, predicted, average='weighted')\n",
    "accuracy_score = accuracy_score(y_test, predicted)\n",
    "recall_score = recall_score(y_test,predicted, average='weighted')\n",
    "precision_score = precision_score(y_test,predicted, average='weighted')\n",
    "confusion_matrix = confusion_matrix(y_test, predicted)\n",
    "print(f\"f1 Score: {f1_score}\\nAccuracy Score:{accuracy_score}\\nRecall Score:{recall_score}\\nPrecision Score:{precision_score}\\n\")\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 Score: 0.42950209418514934\n",
    "\n",
    "Accuracy Score:0.4471997770966843\n",
    "\n",
    "Recall Score:0.4471997770966843\n",
    "\n",
    "Precision Score:0.45357166337232335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a support vector machine using the Action Units of labeled samples from phoebe_AU.csv. Use 5-fold cross-validation on this training set to report the performance. Report your perceived qualitative performance on the unknown labels (e.g. How many appear correct? Provide your own labels as unknown groundtruth to help quantify your results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load the Phoebe dataset\n",
    "phoebe_data = pd.read_csv('Phoebe_AU.csv')\n",
    "\n",
    "# Load Training Data (all records with labels)\n",
    "train_data = phoebe_data[phoebe_data['label'] != 'unknown']\n",
    "labelled_data = train_data['label']\n",
    "train_data = train_data.drop(['file_name', 'label'], axis=1)\n",
    "\n",
    "# Load Test Data (all records without label)\n",
    "test_data = phoebe_data[phoebe_data['label'] == 'unknown']\n",
    "test_data = test_data.drop(['file_name', 'label'], axis=1)\n",
    "\n",
    "# Scale the test data using the same scaler\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "# Initialize and train SVM model\n",
    "svm = SVC()\n",
    "svm.fit(train_data_scaled, labelled_data)\n",
    "\n",
    "cv_scores = cross_val_score(svm, train_data_scaled, labelled_data, cv=5)\n",
    "print(\"CV Scores:\", cv_scores)\n",
    "print(\"Average CV Score:\", cv_scores.mean())\n",
    "\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Test the model\n",
    "test_predictions = svm.predict(test_data_scaled)\n",
    "\n",
    "for i, prediction in zip(test_data.index, test_predictions):\n",
    "    test_data.loc[i, 'label'] = prediction\n",
    "\n",
    "# Print the estimated labels for test data\n",
    "print(\"Estimated labels for records with label value 'unknown':\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like much of the predicted labels are limited by the amount of data available to train on. However, looking at the ones that have been classified into a basic emotion, it seems like it is mostly is accurate. Below are the labels I would associate with the unknown labelled images is:\n",
    "\n",
    "0. surprise (same as model)\n",
    "\n",
    "4. disgust \n",
    "\n",
    "5. sad (same as model)\n",
    "\n",
    "11. happy (same as model)\n",
    "\n",
    "13. happy (same as model)\n",
    "\n",
    "47. angry \n",
    "\n",
    "62. surprise \n",
    "\n",
    "74. happy (same as model)\n",
    "\n",
    "78. sad (same as model)\n",
    "\n",
    "81. happy (same as model)\n",
    "\n",
    "84. disgust \n",
    "\n",
    "93. surprise (same as model)\n",
    "\n",
    "Comparing the self-labelled and the model-labelled versions, the model had a 8/12 = 66% of the classified images correct. Though this would likely improve once the model can classify more distinctions between the surprise, happy, sad with angry and disgust. Most of the discrepency between the incorrect classified images was that the model lacked the distinctions between similar emotions (disgust vs surprise) and (angry vs sad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B\n",
    "### Neural Network\n",
    "\n",
    "Neural Network. Train a neural network using the images from fer2013.csv using Keras. Your first layer should be a Conv2D layer, and the last layers should be a Dense layer followed by a Softmax. Use the Training set for training, PublicTest validation set to avoid overfitting, and the PrivateTest test set for testing. Aim for a minimum validation accuracy of 40% on the Fer2013 validation set. To enhance your model's performance, experiment with various batch sizes and epochs. Incorporate dropout and normalization techniques to further mitigate overfitting and improve generalization. Report precision, recall, accuracy, F1 score, and create a confusion matrix on the test set.\n",
    "\n",
    "Test. Use your trained neural network from Part B.1 and classify the Phoebe unknown image data. Report your perceived performance on the unknown labels, comparing it to the SVM in Part A.2.\n",
    "\n",
    "Fine-tune the Neural Network, and re-classify. Fine-tune your neural network on the Phoebe-face image dataset provided (Hints: use imread() in grayscale to read the images, and freeze early layer weights during fine-tuning). Then, reclassify the images in unknown. Do you think the results improved compared to Part B.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"fer2013.csv\")\n",
    "df['pixels'] = df['pixels'].apply(lambda x: np.array(x.split(), dtype=np.uint8))\n",
    "\n",
    "# convert pixels to input shape and split into separate sets\n",
    "X = np.stack(df['pixels'].values).reshape(-1, 48, 48, 1) / 255.0\n",
    "y = to_categorical(df['emotion'])\n",
    "\n",
    "# splitting data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=df['Usage'], random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)), # merge\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'), # additional convolution stage\n",
    "    MaxPooling2D(pool_size=(2, 2)), # merge\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5), \n",
    "    Dense(7, activation='softmax') # softmax with 7 possible one-hot encodings\n",
    "])\n",
    "\n",
    "# train with 20 epoch cycles and using crossentropy cost function\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=20, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(\"Accuracy:\", test_acc) # validate >40% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model and reporting results\n",
    "predicted = np.argmax(model.predict(X_test), axis=-1) \n",
    "\n",
    "true_positives = np.argmax(y_test, axis=-1)\n",
    "\n",
    "print(classification_report(true_positives, predicted)) # all scores\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix = confusion_matrix(true_positives, predicted)\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=range(7))\n",
    "cm_disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "phoebe_df = pd.read_csv(\"phoebe_AU.csv\")\n",
    "\n",
    "# get image names that are unknown labelled\n",
    "unknown_df = phoebe_df[phoebe_df['label'] == 'unknown']\n",
    "\n",
    "X_unknown = [] # stores the image data for each unknown labelled image\n",
    "for file_name in unknown_df['file_name']:\n",
    "    image = cv2.imread(os.path.join(\"images\", \"unknown\", file_name), cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    image = image / 255.0 # normalizes pixel values\n",
    "    X_unknown.append(image)\n",
    "\n",
    "X_unknown = np.array(X_unknown)\n",
    "\n",
    "# reshape for model inputting\n",
    "X_unknown = X_unknown.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# test the unknown dataset on the model\n",
    "unknown_predictions = model.predict(X_unknown)\n",
    "\n",
    "# converts the one hot encoding to a number label\n",
    "emotion_labels = range(7)\n",
    "predicted_emotions = [emotion_labels[np.argmax(pred)] for pred in unknown_predictions]\n",
    "\n",
    "# update the images with the predicted label\n",
    "unknown_df['predicted_label'] = predicted_emotions\n",
    "print(unknown_df[['file_name', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the neural network designed using keras, we observe labelled:\n",
    "\n",
    "     file_name  predicted_label\n",
    "0     1_01.jpg                4\n",
    "\n",
    "4     4_01.jpg                4\n",
    "\n",
    "5     4_20.jpg                4\n",
    "\n",
    "11    8_01.jpg                3\n",
    "\n",
    "13    9_41.jpg                3\n",
    "\n",
    "47  26_123.jpg                3\n",
    "\n",
    "62   35_42.jpg                5\n",
    "\n",
    "74   41_06.jpg                3\n",
    "\n",
    "78   44_01.jpg                6\n",
    "\n",
    "81   46_03.jpg                3\n",
    "\n",
    "84   48_01.jpg                0\n",
    "\n",
    "93   52_31.jpg                3\n",
    "\n",
    "This is a predominantly high number of classifications to 3 -> happy, however since this model was designed on the data of fer2013.csv and it had labels from 0 to 6, we get some outstanding labels for instance 48_01.jpg\n",
    "\n",
    "The mapping for labels is as follows:\n",
    "\n",
    "1 -> angry\n",
    "\n",
    "2 -> disgust\n",
    "\n",
    "3 -> happy\n",
    "\n",
    "4 -> sad \n",
    "\n",
    "5 -> surprise\n",
    "\n",
    "6 -> unknown\n",
    "\n",
    "Overall, this results in 6/12 = 50% being correctly classified. This is worse than the 66% accurate predictions that were observe in A.2. This NN model was not as consistent with feature distinction, particularly between happy and surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess images into model input shape\n",
    "def load_images(directory='images/', img_height=48, img_width=48):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "    for label in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        for image_file in os.listdir(label_dir):\n",
    "            image_path = os.path.join(label_dir, image_file)\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, (img_width, img_height))\n",
    "            image = image / 255.0 \n",
    "            image_data.append(image)\n",
    "            labels.append(label)\n",
    "    return np.array(image_data), np.array(labels)\n",
    "\n",
    "image_data, labels = load_images()\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "labels = pd.get_dummies(labels).values \n",
    "\n",
    "# split data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(image_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# save the original model and add additional laeyrs to the model\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "model = Sequential([\n",
    "    model,\n",
    "    Flatten(), \n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "_, test_acc = model.evaluate(x_val, y_val)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoebe_df = pd.read_csv(\"phoebe_AU.csv\")\n",
    "\n",
    "# get image names that are unknown labelled\n",
    "unknown_df = phoebe_df[phoebe_df['label'] == 'unknown']\n",
    "\n",
    "X_unknown = [] # stores the image data for each unknown labelled image\n",
    "for file_name in unknown_df['file_name']:\n",
    "    image = cv2.imread(os.path.join(\"images\", \"unknown\", file_name), cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    image = image / 255.0 # normalizes pixel values\n",
    "    X_unknown.append(image)\n",
    "\n",
    "X_unknown = np.array(X_unknown)\n",
    "\n",
    "# reshape for model inputting\n",
    "X_unknown = X_unknown.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# test the unknown dataset on the model\n",
    "unknown_predictions = model.predict(X_unknown)\n",
    "\n",
    "# converts the one hot encoding to a number label\n",
    "emotion_labels = range(7)\n",
    "predicted_emotions = [emotion_labels[np.argmax(pred)] for pred in unknown_predictions]\n",
    "\n",
    "# update the images with the predicted label\n",
    "unknown_df['predicted_label'] = predicted_emotions\n",
    "print(unknown_df[['file_name', 'predicted_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results in B.3, it is seen that the fine tuning of the original model fails to demonstrate improvement as there was an accuracy of 42% on the predicted results vs the analytical results that I was able to observe from the images. This loss in value could be a result from the way the model was appended to. The model was appended to at the softmax level, which may have improved by removing the encoding layer, then adding the additional relu activation layer, before adding another softmax output layer to classify the images in the 5-bit encoding they should be. Unfortunately, due to time and computation constraints, this wasn't able to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n",
    "### Comparison\n",
    "\n",
    "Compare. Compare the results from the 4 models (SVM-Fer2013, SVM-OpenFace, NN-Fer2013, NN-FineTuned) on the Phoebe unknown dataset. Specifically compare the approach with hand-crafted features (SVM-OpenFace) versus neural network extracted features (NN-FineTuned). Choose the one that you think worked best with this dataset. Justify your answer based on the results from Part A and Part B and discuss limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results from all the models, specifically the ones with hand-crafted features from the svm and nn variants, I observe a better result from the SVM. The SVM implementation had a final accuracy of 66% on the unknown dataset labelling, however the same test on the Neural Network variant resulted in a mere 42% accuracy. A limitation of the Neural Network implementation is the variants that need to be tested to achieve an optimal result, as well as the high dependance of the activation layer type and number of layers which greatly impact the processing. With this dataset and perhaps more time spent fine tuning the NN model, it could have been possible to observe a better prediction accuracy due to the control the designer has on the model.\n",
    "\n",
    "The results from part A, particularly training on fer2013 showed a better scores overall, particularly viewing the confusion matrix. There were more cases along the diagonal (true positives) of the confusion matrix on the SVM compared to NN. The performance of the SVM also carried through to the training and testing using the phoebe dataset. The SVM provided a 66% accuracy on the phoebe unknown dataset, whereas the NN implementation got 42% of them right. \n",
    "\n",
    "Now that we've discussed the results, and some limitations of the models, it's important to also mention limitations from the controllers perspective. Particularly the person who labels the unknown data themselves, in the case of this assignment, it's me. It is important to note that I may perceive an emotion incorrectly than that another individual would. This is especially true for emotions that share similar action units, such as surprise and happy. This was also a common occurence in the actual model predictions, where they confused happy and surprise, similarly, angry and disgust were common mistakes in the predictions. Ultimately, these model predictions should be a small part of a larger system which can account for these nuances, especially when these distinctions impact the way a system behaves towards the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
